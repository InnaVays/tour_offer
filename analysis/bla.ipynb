{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, norm\n",
    "from statsmodels.stats.power import NormalIndPower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/processed_experiment_results.csv\")\n",
    "\n",
    "# Overview of Data\n",
    "print(\"\\n Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n Summary Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Overall summary statistics\n",
    "summary_stats = df.groupby(\"strategy\").agg(\n",
    "    clicks_mean=(\"clicks\", \"mean\"),\n",
    "    clicks_median=(\"clicks\", \"median\"),\n",
    "    interest_mean=(\"high_interest\", \"mean\"),\n",
    "    interest_median=(\"high_interest\", \"median\"),\n",
    "    session_length_mean=(\"session_length\", \"mean\"),\n",
    "    session_length_median=(\"session_length\", \"median\"),\n",
    "    count=(\"session_id\", \"count\")\n",
    ")\n",
    "\n",
    "# Show summary statistics\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clicks Distribution by Strategy\n",
    "plt.figure(figsize=(9,3))\n",
    "sns.boxplot(data=df, x=\"strategy\", y=\"clicks\")\n",
    "plt.title(\"Clicks Distribution by Strategy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Interest Actions by Strategy\n",
    "plt.figure(figsize=(9,3))\n",
    "sns.boxplot(data=df, x=\"strategy\", y=\"high_interest\")\n",
    "plt.title(\"Interest Actions by Strategy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Session Length Distribution\n",
    "plt.figure(figsize=(9,3))\n",
    "sns.boxplot(data=df, x=\"strategy\", y=\"session_length\")\n",
    "plt.title(\"Session Length by Strategy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_sample_size(baseline_rate, mde=0.05, alpha=0.05, power=0.8):\n",
    "    \"\"\"\n",
    "    Calculates the minimum required sample size per group for detecting a given Minimum Detectable Effect (MDE).\n",
    "    \n",
    "    Args:\n",
    "        baseline_rate : The current conversion rate (CTR or interest rate).\n",
    "        mde : The minimum detectable effect (default = 5% improvement).\n",
    "        alpha : Significance level (default = 0.05).\n",
    "        power : Statistical power (default = 0.8).\n",
    "    \n",
    "    Returns:\n",
    "        int: Minimum required sample size per group.\n",
    "    \"\"\"\n",
    "    # Convert percentages to proportions\n",
    "    p0 = baseline_rate\n",
    "    p1 = p0 * (1 + mde)  # Expected rate after improvement\n",
    "\n",
    "    # Compute effect size (Cohen's h)\n",
    "    effect_size = 2 * np.arcsin(np.sqrt(p1)) - 2 * np.arcsin(np.sqrt(p0))\n",
    "\n",
    "    # Calculate sample size per group\n",
    "    analysis = NormalIndPower()\n",
    "    sample_size = analysis.solve_power(effect_size, power=power, alpha=alpha, ratio=1, alternative=\"two-sided\")\n",
    "\n",
    "    return int(np.ceil(sample_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Assume a baseline CTR of 10% (0.10)\n",
    "baseline_ctr = 0.10  # 10% click-through rate\n",
    "required_sample = calculate_sample_size(baseline_ctr)\n",
    "\n",
    "print(f\"Minimum sample size per group: {required_sample}\")\n",
    "\n",
    "session_counts = df.groupby(['strategy'])['strategy'].count()\n",
    "\n",
    "session_counts = session_counts.to_frame(name=\"observed_sessions\")  # Convert series to DataFrame\n",
    "session_counts[\"sufficient_data\"] = session_counts[\"observed_sessions\"] >= required_sample\n",
    "\n",
    "print('Number of samples in a group exceeds required minimum: ')\n",
    "print(session_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AB Test (against strategy_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-Test (Studentâ€™s T-Test)\n",
    "\n",
    "A **T-test** compares the means of two groups and determines whether the difference is statistically significant or just random variation. The variance in user behavior (clicks, interest event...) might not be known.\n",
    "\n",
    "Compare the mean metric score (number of clickes, events, session length) between strategy 0 (no treatment) and other strategies.\n",
    "\n",
    "The p-value tells if the difference is statistically significant:\n",
    "- If p < 0.05, we reject the null hypothesis (Hâ‚€) and say the strategy makes a difference.\n",
    "- If p â‰¥ 0.05, we fail to reject Hâ‚€ and assume thereâ€™s no real effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score Test\n",
    "\n",
    "A Z-Test is similar to a T-Test, but it assumes a large sample size and known variance. Z-Score indicatess how far away our observed result is from the expected mean in terms of standard deviations.\n",
    "\n",
    "Z-Score Test calculates how many standard deviations away the treatment is from strategy_0 (control).\n",
    "The Z-score is converted into a p-value, which tells us whether the difference is statistically significant.\n",
    "\n",
    "If the sample is big enough (n > 30), the **Central Limit Theorem** allows to use the normal distribution.\n",
    "More stable results for large experiments (thousands of sessions).\n",
    "\n",
    "If sample size > 30, a Z-test is more accurate than a T-test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_z_test(group1, group2, metric):\n",
    "    # Compute means and standard deviations\n",
    "    x1, x2 = df[df[\"strategy\"] == group1][metric].mean(), df[df[\"strategy\"] == group2][metric].mean()\n",
    "    s1, s2 = df[df[\"strategy\"] == group1][metric].std(), df[df[\"strategy\"] == group2][metric].std()\n",
    "    n1, n2 = df[df[\"strategy\"] == group1][metric].count(), df[df[\"strategy\"] == group2][metric].count()\n",
    "\n",
    "    # Compute Z-score\n",
    "    se = np.sqrt((s1**2 / n1) + (s2**2 / n2))\n",
    "    z_score = (x1 - x2) / se\n",
    "    p_value = 2 * (1 - norm.cdf(abs(z_score)))  # Two-tailed test\n",
    "\n",
    "    return z_score, p_value\n",
    "\n",
    "def run_t_test(group1, group2, metric):\n",
    "    stat, p_value = ttest_ind(df[df[\"strategy\"] == group1][metric], \n",
    "                              df[df[\"strategy\"] == group2][metric], \n",
    "                              equal_var=False)\n",
    "    return stat, p_value\n",
    "\n",
    "def generate_test_report( baseline_strategy ):\n",
    "    results = []\n",
    "\n",
    "    for strategy in strategies:\n",
    "        if strategy == baseline_strategy:\n",
    "            continue  \n",
    "\n",
    "        for metric in metrics:\n",
    "            t_stat, t_p_value = run_t_test(baseline_strategy, strategy, metric)\n",
    "            z_stat, z_p_value = run_z_test(baseline_strategy, strategy, metric)\n",
    "            results.append([baseline_strategy, strategy, metric, t_stat, t_p_value, z_stat, z_p_value])\n",
    "\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    t_test_results_df = pd.DataFrame(results, columns=[\"Baseline\", \"Compared Strategy\", \"Metric\", \"T-Statistic\", \"P-Value\"])\n",
    "\n",
    "    # Print results as a clean table\n",
    "    print(\"\\nðŸ“Š **T-Test Results Against Baseline (strategy_0)**\")\n",
    "    print(t_test_results_df.to_string(index=False))\n",
    "\n",
    "    return t_test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
